{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 04 - Applied ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Propensity score matching\n",
    "\n",
    "In this exercise, you will apply [propensity score matching](http://www.stewartschultz.com/statistics/books/Design%20of%20observational%20studies.pdf), which we discussed in lecture 5 (\"Observational studies\"), in order to draw conclusions from an observational study.\n",
    "\n",
    "We will work with a by-now classic dataset from Robert LaLonde's study \"[Evaluating the Econometric Evaluations of Training Programs](http://people.hbs.edu/nashraf/LaLonde_1986.pdf)\" (1986).\n",
    "The study investigated the effect of a job training program (\"National Supported Work Demonstration\") on the real earnings of an individual, a couple of years after completion of the program.\n",
    "Your task is to determine the effectiveness of the \"treatment\" represented by the job training program.\n",
    "\n",
    "\n",
    "#### Dataset description\n",
    "\n",
    "- `treat`: 1 if the subject participated in the job training program, 0 otherwise\n",
    "- `age`: the subject's age\n",
    "- `educ`: years of education\n",
    "- `race`: categorical variable with three possible values: Black, Hispanic, or White\n",
    "- `married`: 1 if the subject was married at the time of the training program, 0 otherwise\n",
    "- `nodegree`: 1 if the subject has earned no school degree, 0 otherwise\n",
    "- `re74`: real earnings in 1974 (pre-treatment)\n",
    "- `re75`: real earnings in 1975 (pre-treatment)\n",
    "- `re78`: real earnings in 1978 (outcome)\n",
    "\n",
    "If you want to brush up your knowledge on propensity scores and observational studies, we highly recommend Rosenbaum's excellent book on the [\"Design of Observational Studies\"](http://www.stewartschultz.com/statistics/books/Design%20of%20observational%20studies.pdf). Even just reading the first chapter (18 pages) will help you a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "import networkx as nx\n",
    "from IPython.display import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('lalonde.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. A naive analysis\n",
    "\n",
    "Compare the distribution of the outcome variable (`re78`) between the two groups, using plots and numbers.\n",
    "To summarize and compare the distributions, you may use the techniques we discussed in lectures 4 (\"Read the stats carefully\") and 6 (\"Data visualization\").\n",
    "\n",
    "What might a naive \"researcher\" conclude from this superficial analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "We divide the dataset into two groups: treated and non-treated (according to the indicator variable *treat*), and we try to evaluate whether there are differences in earning on the outcome variable (*re78*):\n",
    "- we compute and plot the two distributions.\n",
    "- we analyse the raw data by observing mean, median and standard deviation of the earnings and the size of the dataset as well.\n",
    "- we apply the Kolmogorov–Smirnov test to check whether we can reject the null hypothesis (null hypothesis: the earning of the treated and the non-treated group are generated by the same distribution).\n",
    "\n",
    "\n",
    "TODO: write about zeros\n",
    "\n",
    "TODO: write about fitting and histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyse_earnings_distribution(dataset):\n",
    "    \"\"\" Split the data and analyse the earnings:\n",
    "    -Compute and plot the distributions.\n",
    "    -Compute the descriptive statistics to summarize the earnings.\n",
    "    -Compute the pvalue either with the Kolmogorov–Smirnov test. or with the Wilcoxon test.\n",
    "    \"\"\"\n",
    "    outcome_treated = dataset[dataset['treat'] == 1]['re78']\n",
    "    outcome_nontreated = dataset[dataset['treat'] == 0]['re78']\n",
    "    \n",
    "    bins = np.linspace(dataset['re78'].min(), dataset['re78'].max(), 30)\n",
    "    colors = sns.color_palette()\n",
    "    sns.distplot(outcome_treated, kde=False, fit=stats.expon, bins=bins, color=colors[0], fit_kws={'color': colors[0]})\n",
    "    ax = sns.distplot(outcome_nontreated, kde=False, fit=stats.expon, bins=bins, color=colors[1], fit_kws={'color': colors[1]})\n",
    "    ax.set_xlim(0,)\n",
    "    plt.grid()\n",
    "    plt.legend(['Treated', 'Non-treated'])\n",
    "    plt.show()\n",
    "\n",
    "    description = pd.DataFrame()\n",
    "    description['Earnings (treated)'] = outcome_treated.describe()\n",
    "    description['Earnings (non-treated)'] = outcome_nontreated.describe()\n",
    "    \n",
    "    description['Earnings (treated)'] = description['Earnings (treated)'].astype(\"int\")\n",
    "    description['Earnings (non-treated)'] = description['Earnings (non-treated)'].astype(\"int\")\n",
    "    display(description)\n",
    "    \n",
    "    print(stats.ks_2samp(outcome_nontreated, outcome_treated))\n",
    "    if len(outcome_nontreated) == len(outcome_treated):\n",
    "        print(stats.wilcoxon(outcome_nontreated, outcome_treated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analyse_earnings_distribution(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- It appears that the two distributions are heavy-tailed and they also seem not to differ significantly. Since we are dealing with a heavy-tailed distribution, computing the mean does not make much sense since it is heavily affected by the few high values. Therefore, it is better to consider the more robust median to compare the earnings.\n",
    "- By inspecting the median, it seems that the non-treated group have slightly higher earnings than the treated group. However, we have to argue whether this difference is significant. In addition, we can see that the population sizes are unbalanced: we have more individuals in the non-treatment group (429) than the treatment group (185). TODO: comment std (https://math.stackexchange.com/questions/260617/how-to-determine-if-standard-deviation-is-high-low) ?\n",
    "- The latter observations lead us to hypothesize that the two outcomes are drawn from the same probability distribution. We decided to run a test to verify we can not indeed reject this hypothesis $H_0$ (null hypothesis). As can be seen from the histograms above, the distributions are certainly not Gaussian, but resemble a power law. Therefore, we decided to adopt the two-sided K-S (Kolmogorov-Smirnov) test, which is a non-parametric test, and as such does not require the assumption that the data is normally-distributed. As can be seen, the p-value is not low enough to reject the null hypothesis. We are assuming a significance level $\\alpha = 0.05$, which means that we should reject the null hypothesis only if $p < 0.05$. We can conclude that there is not sufficient evidence that the treatment is effective in improving the income.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "Our argument is still weak: the fact that we cannot reject the null hypothesis (the two earnings distributions are equal) does not mean that the null hypothesis is correct. For this reason, in the next part of the homework we will try to further bound the p-value and gain more evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. A closer look at the data\n",
    "\n",
    "You're not naive, of course (and even if you are, you've learned certain things in ADA), so you aren't content with a superficial analysis such as the above.\n",
    "You're aware of the dangers of observational studies, so you take a closer look at the data before jumping to conclusions.\n",
    "\n",
    "For each feature in the dataset, compare its distribution in the treated group with its distribution in the control group, using plots and numbers.\n",
    "As above, you may use the techniques we discussed in class for summarizing and comparing the distributions.\n",
    "\n",
    "What do you observe?\n",
    "Describe what your observations mean for the conclusions drawn by the naive \"researcher\" from his superficial analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "Before drawing any conclusion we have to assess that the assignment of participants to the treatment is random, i.e. ideally it should not be possible to determine if an individual has been assigned to the treatment by just looking at their features.\n",
    "\n",
    "For this purpose we will first plot the distribution of all the feature of the individuals after divideng them in treated and non-treated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_features(dataset):\n",
    "    n_cols, n_rows = 2, 4\n",
    "    width, heigth = n_cols*5, n_rows*5\n",
    "    \n",
    "    fig, axs = plt.subplots(n_rows, n_cols)\n",
    "    fig.set_size_inches(width, heigth)\n",
    "    \n",
    "    treated = dataset[dataset['treat'] == 1]\n",
    "    nontreated = dataset[dataset['treat'] == 0]\n",
    "    \n",
    "    continuous = ['age', 'educ', 're74', 're75']\n",
    "    for i, feature in enumerate(continuous):\n",
    "        r, c = int(i/2), i%2\n",
    "        bins = np.linspace(dataset[feature].min(), dataset[feature].max(), 20)\n",
    "        sns.distplot(treated[feature], bins=bins, ax=axs[r][c])\n",
    "        sns.distplot(nontreated[feature], bins=bins, ax=axs[r][c])\n",
    "        axs[r][c].legend(['Treated', 'Non-treated'])\n",
    "        axs[r][c].set_ylabel('Density')\n",
    "\n",
    "    discretes = ['black', 'hispan', 'married', 'nodegree']\n",
    "    contrary = {'black': 'not black', 'hispan': 'not hispan', 'married': 'unmarried', 'nodegree': 'with degree'}\n",
    "    for i, feature in enumerate(discretes):\n",
    "        r, c = int(i/2)+2, i%2\n",
    "        count_all = dataset.groupby(['treat'])[feature].count()\n",
    "        (dataset.groupby(['treat', feature])[feature].count()/count_all).unstack()\\\n",
    "            .plot.bar(stacked=True, ax=axs[r][c])\n",
    "        \n",
    "\n",
    "        axs[r][c].legend([contrary[feature], feature], loc = 'lower center')\n",
    "        axs[r][c].set_xticklabels(['Non-treated', 'Treated'])\n",
    "        axs[r][c].set_ylabel('Proportion within group')\n",
    "        axs[r][c].set_xlabel('')\n",
    "        \n",
    "        for tick in axs[r][c].get_xticklabels():\n",
    "            tick.set_rotation(45)\n",
    "        \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_features(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations.** The most noticeably differences are in the following features:\n",
    "- **re74, re75**: the treated set has a higher number of individuals whose earning was 0 during 1974 and 1975.\n",
    "- **black**: ~80% of black individuals in the treated set, ~20% of black individuals in the non-treated set.\n",
    "- **married**: ~20% of married individuals in the treated set, ~50% of of married individuals in the treated set.\n",
    "\n",
    "**Conclusion.**\n",
    "\n",
    "Just by looking at the distributions of the features we notice great differences which indicate that the assignment of the individuals to the treatment most probabily was not random. The next task will help quantify how much it is possible to predict whether an individual has been assigned to the treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. A propensity score model\n",
    "\n",
    "Use logistic regression to estimate propensity scores for all points in the dataset.\n",
    "You may use `sklearn` to fit the logistic regression model and apply it to each data point to obtain propensity scores:\n",
    "\n",
    "```python\n",
    "from sklearn import linear_model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "```\n",
    "\n",
    "Recall that the propensity score of a data point represents its probability of receiving the treatment, based on its pre-treatment features (in this case, age, education, pre-treatment income, etc.).\n",
    "To brush up on propensity scores, you may read chapter 3.3 of the above-cited book by Rosenbaum or [this article](https://drive.google.com/file/d/0B4jctQY-uqhzTlpBaTBJRTJFVFE/view).\n",
    "\n",
    "Note: you do not need a train/test split here. Train and apply the model on the entire dataset. If you're wondering why this is the right thing to do in this situation, recall that the propensity score model is not used in order to make predictions about unseen data. Its sole purpose is to balance the dataset across treatment groups.\n",
    "(See p. 74 of Rosenbaum's book for an explanation why slight overfitting is even good for propensity scores.\n",
    "If you want even more information, read [this article](https://drive.google.com/file/d/0B4jctQY-uqhzTlpBaTBJRTJFVFE/view).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "We use the logistic regression so to assign to each individual a propensity score which indicates the probability of being assigned to the treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic = linear_model.LogisticRegression(C=1e9)\n",
    "X = dataset[['age', 'educ', 'black', 'hispan', 'married', 'nodegree', 're74', 're75']]\n",
    "y = dataset['treat']\n",
    "logistic.fit(X, y)\n",
    "dataset['propensity'] = logistic.predict_proba(X)[:, 1]\n",
    "\n",
    "print(\"Accuracy of predictions:\", logistic.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"This random subset shows that we could correctly predict whether these 5 individuals were assigned to the \\ntreatment.\")\n",
    "sample = dataset.sample(n=5, random_state=5)[[\"treat\", \"propensity\"]]\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations.**\n",
    "\n",
    "As expected, this computation shows that with a simple 1-degree model we could predict with 80% accuracy whether an individual has been assigned to the treatment. Therefore, we have to balance the two sets before being able to draw credible conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Balancing the dataset via matching\n",
    "\n",
    "Use the propensity scores to match each data point from the treated group with exactly one data point from the control group, while ensuring that each data point from the control group is matched with at most one data point from the treated group.\n",
    "(Hint: you may explore the `networkx` package in Python for predefined matching functions.)\n",
    "\n",
    "Your matching should maximize the similarity between matched subjects, as captured by their propensity scores.\n",
    "In other words, the sum (over all matched pairs) of absolute propensity-score differences between the two matched subjects should be minimized.\n",
    "\n",
    "After matching, you have as many treated as you have control subjects.\n",
    "Compare the outcomes (`re78`) between the two groups (treated and control).\n",
    "\n",
    "Also, compare again the feature-value distributions between the two groups, as you've done in part 2 above, but now only for the matched subjects.\n",
    "What do you observe?\n",
    "Are you closer to being able to draw valid conclusions now than you were before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def min_weight_matching(dataset, max_weight=1):\n",
    "    \"\"\" Build a graph in which the nodes are the individuals. Individuals belonging two different sets\n",
    "    (train and non-train set) are connected by weighted edges. The weight is the absolute value of the \n",
    "    difference between the propensity scores of the two individuals. Only the edges whose weight \n",
    "    is under max_weight are kept. \n",
    "    Returns the datatset after dropping all the non-matched entries.\n",
    "    \"\"\"\n",
    "    \n",
    "    treated = dataset[dataset['treat'] == 1].index\n",
    "    nontreated = dataset[dataset['treat'] == 0].index\n",
    "\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for i in dataset.index:\n",
    "        G.add_node(i)\n",
    "\n",
    "    for i in treated:\n",
    "        for j in nontreated:\n",
    "            weight = abs(dataset['propensity'].loc[i] - dataset['propensity'].loc[j])\n",
    "            if weight < max_weight:\n",
    "                G.add_edge(i, j, weight=-weight)\n",
    "\n",
    "    matching = nx.algorithms.max_weight_matching(G, maxcardinality=True)\n",
    "    \n",
    "    # build the dataframe to show the distances of the matched points\n",
    "    matches = pd.DataFrame()\n",
    "    matches[\"id_1\"] = matching.keys()\n",
    "    matches[\"id_2\"] = matching.values()\n",
    "    prop1 = pd.Series(dataset.loc[matches[\"id_1\"]].propensity.tolist())\n",
    "    prop2 = pd.Series(dataset.loc[matches[\"id_2\"]].propensity.tolist())\n",
    "    matches[\"edge weight\"] = (prop1 - prop2).abs().round(decimals=4)\n",
    "    matches = matches.sort_values(by=\"edge weight\")\n",
    "    matches = matches[matches['id_1'] > matches['id_2']]\n",
    "\n",
    "    print('Found optimal matching with total weight', matches[\"edge weight\"].sum())\n",
    "    print('Number of matches:', matches.shape[0])\n",
    "    \n",
    "    print(\"These are the 5 best matches:\")\n",
    "    display(matches.head())\n",
    "    print(\"These are the 5 worse matches:\")\n",
    "    display(matches.tail())\n",
    "    \n",
    "    matches[\"edge weight\"].reset_index()[\"edge weight\"].hist(bins=50)\n",
    "    plt.title(\"Distribution of the matches\")\n",
    "    plt.ylabel(\"Number of matches\")\n",
    "    plt.xlabel(\"Weight of the edge\")\n",
    "\n",
    "    dataset_balanced = dataset.loc[matching.keys()]\n",
    "    return dataset_balanced #, matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_balanced = min_weight_matching(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analyse_earnings_distribution(dataset_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_features(dataset_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "We can notice that the features are more equally distributed among the two sets, in particular re74 and re75 show less discrepancy in the number of individuals with 0 earnings than before. However, we are still not satisfied enough since there are still datapoints which have been matched but have not a very similar propensity score. This is due to the fact that the algorithm matches all the 185 entries in the treated set with 185 entries in in the other set, even if they have not a similar score. For example, we can notice that the distribution of the black people (**TODO and age**) is still unbalanced. We take care of this problem in the next task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Balancing the groups further\n",
    "\n",
    "Based on your comparison of feature-value distributions from part 4, are you fully satisfied with your matching?\n",
    "Would you say your dataset is sufficiently balanced?\n",
    "If not, in what ways could the \"balanced\" dataset you have obtained still not allow you to draw valid conclusions?\n",
    "\n",
    "Improve your matching by explicitly making sure that you match only subjects that have the same value for the problematic feature.\n",
    "Argue with numbers and plots that the two groups (treated and control) are now better balanced than after part 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "Since the **'black'** feature is still not balanced among the two sets, we decided to repeat the matching algorithm but changing the graph. This time we create a weighted edge between two individuals only if both are black or both are not black (and they belong to different sets). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "treated = dataset[dataset['treat'] == 1].index\n",
    "nontreated = dataset[dataset['treat'] == 0].index\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for i in dataset.index:\n",
    "    G.add_node(i)\n",
    "\n",
    "for i in treated:\n",
    "    for j in nontreated:\n",
    "        if dataset['black'].loc[i] == dataset['black'].loc[j]:\n",
    "            G.add_edge(i, j, weight=-abs(dataset['propensity'].loc[i] - dataset['propensity'].loc[j]))\n",
    "\n",
    "matching = nx.algorithms.max_weight_matching(G, maxcardinality=True)\n",
    "\n",
    "weight = 0\n",
    "for a, b in matching.items():\n",
    "    weight += G[a][b]['weight']\n",
    "print('Found optimal matching with total difference', -weight/2)\n",
    "dataset_balanced = dataset.loc[matching.keys()]\n",
    "print('Population size of match:', len(dataset_balanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "treated_group = []\n",
    "nontreated_group = []\n",
    "for a, b in matching.items():\n",
    "    if dataset['treat'].loc[a] == 1:\n",
    "        treated_group.append(a)\n",
    "        nontreated_group.append(b)\n",
    "\n",
    "treated_group = dataset.loc[treated_group]\n",
    "nontreated_group = dataset.loc[nontreated_group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stats.wilcoxon(treated_group['re78'], nontreated_group['re78'])\n",
    "analyse_earnings_distribution(dataset_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_features(dataset_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trial 2: keep only the edges whose weight is under a threshold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_balanced = min_weight_matching(dataset, max_weight=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analyse_earnings_distribution(dataset_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_features(dataset_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just check black ratios in treated and non treated sets (delete)\n",
    "treated = dataset_balanced[dataset_balanced.treat==1]\n",
    "nontreated = dataset_balanced[dataset_balanced.treat==0]\n",
    "treated[treated.black == 1].count()[0]/treated.count()[0], nontreated[nontreated.black == 1].count()[0]/nontreated.count()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. A less naive analysis\n",
    "\n",
    "Compare the outcomes (`re78`) between treated and control subjects, as you've done in part 1, but now only for the matched dataset you've obtained from part 5.\n",
    "What do you conclude about the effectiveness of the job training program?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analyse_earnings_distribution(dataset_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "There is no strong evidence to reject the null hypothesys, therefore we cannot conclude that the training is useless. **TODO** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Applied ML\n",
    "\n",
    "We are going to build a classifier of news to directly assign them to 20 news categories. Note that the pipeline that you will build in this exercise could be of great help during your project if you plan to work with text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the 20newsgroup dataset. It is, again, a classic dataset that can directly be loaded using sklearn ([link](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)).  \n",
    "[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), short for term frequency–inverse document frequency, is of great help when if comes to compute textual features. Indeed, it gives more importance to terms that are more specific to the considered articles (TF) but reduces the importance of terms that are very frequent in the entire corpus (IDF). Compute TF-IDF features for every article using [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Then, split your dataset into a training, a testing and a validation set (10% for validation and 10% for testing). Each observation should be paired with its corresponding label (the article category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the *20newsgroup* dataset using the built-in function by sklearn. Since we will split the dataset manually, we download everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_loaded = fetch_20newsgroups(subset='all')\n",
    "newsgroups_loaded['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of newsgroup posts divided by categories. In this dataset, 20 categories (shown above) are available. For each post in ```newsgroups['data'][i]```, we have the corresponding ground truth in ```newsgroups['target'][i]```, represented as an index between 0 and 19. The full name of the category can be retrieved simply by looking at ```newsgroups['target_name'][i]```.\n",
    "\n",
    "We now split the dataset into three groups:\n",
    "- **Training set (80%):** it will be used for training the model.\n",
    "- **Validation set (10%):** it will be used for experimenting with the model and tuning its hyperparameters, so as to obtain the best accuracy. The model will never be trained using this set.\n",
    "- **Test set (10%):** it will be evaluated only once, when the model is ready. This will determine the final accuracy of our model.\n",
    "\n",
    "This scheme allows us to evaluate whether the model generalizes or overfits the dataset. Generally speaking, we want our model to generalize well (that is, be able to classify unseen samples correctly) instead of just memorizing the input. A better approach would be to use *k-fold cross-validation*, i.e. split the dataset into $k$ parts, train it $k$ times on $k - 1$ parts, and validate it on the remaining parts. This would yield a more accurate estimate of the prediction error. However, we employed a static validation set since it is explicitly asked to do so (additionally, k-fold is more computationally expensive).\n",
    "\n",
    "Prior to splitting the dataset, we randomly shuffle the records. Throughout the rest of the homework, we will use a fixed seed (```random_state=0```) for all random operations, so as to ensure reproducibility of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups = pd.DataFrame()\n",
    "newsgroups['data'] = newsgroups_loaded['data']\n",
    "newsgroups['target'] = newsgroups_loaded['target']\n",
    "shuffled = newsgroups.sample(frac=1, random_state=0)\n",
    "shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = shuffled.shape[0]\n",
    "newsgroups_train = shuffled[:int(n*0.8)]\n",
    "newsgroups_validation = shuffled[int(n*0.8):int(n*0.9)]\n",
    "newsgroups_test = shuffled[int(n*0.9):]\n",
    "print('# samples in the training set:', newsgroups_train.shape[0])\n",
    "print('# samples in the validation set:', newsgroups_validation.shape[0])\n",
    "print('# samples in the test set:', newsgroups_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed with converting each document to a feature vector. Machine learning algorithms require vector representations for their inputs, that is, each sample must be a point in $\\mathbb{R}^d$ space. Text, on the other hand, is not well suited to be represented as a vector, and therefore some form of pre-processing is required. The idea is that documents that are similar in content/semantics are also close among each other in their vector space.\n",
    "\n",
    "We perform the feature extraction using ```TfidfVectorizer```, which extracts *tf-idf* features. In a way that resembles a *bag-of-words* approach, words are assigned a weight that is proportional to the number of times they appear inside a document. Additionally, words that appear in many documents (e.g. \"this\", \"it\", \"the\", \"is\") are penalized.\n",
    "\n",
    "The algorithm works by first tokenizing each document (using ```CountVectorizer```), that is, extracting the words and getting the count for each word. Afterwards, features are transformed with ```TfidfTransformer```, which produces the final *tf-idf* features. The result is a sparse matrix that can be fed to a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_tr = vectorizer.fit_transform(newsgroups_train['data'])\n",
    "X_va = vectorizer.transform(newsgroups_validation['data'])\n",
    "X_te = vectorizer.transform(newsgroups_test['data'])\n",
    "\n",
    "y_tr = newsgroups_train['target'].values\n",
    "y_va = newsgroups_validation['target'].values\n",
    "y_te = newsgroups_test['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we fitted the vectorizer only on the training set, *after* splitting the dataset. The validation and test sets are transformed using the same model, as if they were unseen samples. This is the correct approach, since the full pipeline (vectorizer + classifier) must be trained using the training set alone. In a realistic scenario, the validation/test sets should only be used for evaluating the model. Fitting the vectorizer on the latter would bias the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Train a random forest on your training set. Try to fine-tune the parameters of your predictor on your validation set using a simple grid search on the number of estimator \"n_estimators\" and the max depth of the trees \"max_depth\". Then, display a confusion matrix of your classification pipeline. Lastly, once you assessed your model, inspect the `feature_importances_` attribute of your random forest and discuss the obtained results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to keep the number of estimators constant, and run the grid search only on the maximum depth. As a rule of thumb, the number of estimators in a random forest should be set to a high value (e.g. 100 to 1000) and not be touched anymore, since it can only improve the result. As described by the original authors of the algorithm, [\"you can run as many trees as you want\"](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#remarks). Many researchers claim that 100 to 300 estimators (i.e. trees) are more than enough for almost all cases. As a result, the choice of this hyperparameter is just a trade-off between training time and accuracy, given that after a certain threshold the accuracy no longer improves.\n",
    "\n",
    "Another advantage of having a large number of trees is that the result is more stable. Random forest use *bagging*, which means that the final result is the average of the outcomes of many simple models. If a low number of trees is used, the result will tend to fluctuate due to the high variance, whereas with many trees the result will be more consistent. This can be observed by running a k-fold cross validation (which is not done here): the higher the number of trees is, the lower the standard deviation of the error/accuracy will be.\n",
    "\n",
    "We set the number of estimators to 300, which requires a lot of training time, but produces a very stable estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_estimators = 10\n",
    "num_threads = multiprocessing.cpu_count()\n",
    "print('Random forests will be trained using', num_threads, 'CPU cores and', num_estimators, 'estimators')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal tree depth will be searched between 5 and 100 (in steps of 5). For each step, we train the model on the training set and evaluate the accuracy on the validation set. The aim, of course, is to maximize the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "depths = np.arange(5, 100, 5)\n",
    "tr_accuracies = []\n",
    "va_accuracies = []\n",
    "for depth in depths:\n",
    "    clf = RandomForestClassifier(max_depth=depth, random_state=0, n_estimators=num_estimators, n_jobs=num_threads)\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    tr_accuracies.append(clf.score(X_tr, y_tr))\n",
    "    va_accuracies.append(clf.score(X_va, y_va))\n",
    "    print('Depth:', depth, tr_accuracies[-1], va_accuracies[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also plot the comparison between training error and validation error, according to the tree depth. While the validation accuracy is always lower than the training accuracy (which is normal), the trend reveals that a higher tree depth tends to improve the result (at least, in our scenario)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(depths[:len(tr_accuracies)], tr_accuracies)\n",
    "plt.plot(depths[:len(va_accuracies)], va_accuracies)\n",
    "plt.legend(['Training set', 'Validation set'])\n",
    "plt.ylabel('Accuracy')\n",
    "_ = plt.xlabel('Maximum tree depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the final model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_depth = depths[np.argmax(va_accuracies)]\n",
    "print('The best tree depth is', best_depth)\n",
    "clf = RandomForestClassifier(max_depth=best_depth, random_state=0, n_estimators=num_estimators, n_jobs=num_threads)\n",
    "clf.fit(X_tr, y_tr)\n",
    "print('Test accuracy:', clf.score(X_te, y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that the training error is higher than the validation/test error. This is normal, as the model tends to slightly overfit the training set. The important outcome is that the random forest generalizes well to unseen data, which seems the case.\n",
    "\n",
    "We now plot the so-called *confusion matrix*, which allows us to analyze the behaviour of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, cmap=plt.cm.Blues):\n",
    "    \n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title('Normalized confusion matrix', fontsize=20)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.tick_params(labelsize=15)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j],  '.2f'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=18)\n",
    "    plt.xlabel('Predicted label', fontsize=18)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_te, clf.predict(X_te))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(conf_matrix, classes=newsgroups_loaded['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the figure, the model seems to predict relatively well. Most errors are related to religion categories, which are closely related among each other.\n",
    "\n",
    "Another common practice in machine learning consists in interpreting the model, that is, visualizing the features that are learned, and how they affect each prediction label. In order to do this, we analyze the field `feature_importances_`, which, as the name suggests, provides us with the importance of each feature (a real value between 0 and 1 that represents the impact on the final prediction). In our case, each feature corresponds to a word, as returned by ```TfidfVectorizer```. Similarly, we can convert a feature back to a textual word by using the inverse map.\n",
    "We now show the 10 most important features of the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "important_features = pd.DataFrame(clf.feature_importances_).sort_values(by=0, ascending=False).head(10)\n",
    "\n",
    "mapping = vectorizer.get_feature_names()\n",
    "important_features.index = important_features.index.map(lambda x: mapping[x])\n",
    "important_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not very suggestive. We can see the words that have the highest impact on the model, but we cannot really interpret how they affect the result, as the model predicts 20 classes. We need to convert our interpretation problem to a binary classification problem. The solution is to build a [one-vs-rest](https://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest) model, that is, for each label $k$ we train a model where $y_i=1$ if $y_i==k$ and $0$ otherwise. This way, we can get useful feature importances for each category of posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "series = []\n",
    "for i, name in enumerate(newsgroups_loaded['target_names']):\n",
    "    clf = RandomForestClassifier(max_depth=best_depth, random_state=0, n_estimators=num_estimators, n_jobs=num_threads)\n",
    "    clf.fit(X_tr, y_tr == i)\n",
    "    important_features = pd.DataFrame(clf.feature_importances_).sort_values(by=0, ascending=False).head(10)\n",
    "    important_features.index = important_features.index.map(lambda x: mapping[x])\n",
    "    important_features.index.name = name\n",
    "    series.append(important_features.reset_index()[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now show the top 10 words for each category. We choose not to display the raw value of the importance, as they are not really easy to interpet. In simple words, if these words appear in a newsgroup post, they will tend to shift the attention of the classifier towards the respective class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.concat(series, axis=1)\n",
    "result.index = range(1, 11)\n",
    "result.index.name = 'rank'\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The features certainly make sense. For instance, we can observe that categories related to religion contain words commonly associated with religion. Likewise, categories related to hardware/software contain domain-specific words."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

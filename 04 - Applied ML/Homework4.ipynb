{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 04 - Applied ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Propensity score matching\n",
    "\n",
    "In this exercise, you will apply [propensity score matching](http://www.stewartschultz.com/statistics/books/Design%20of%20observational%20studies.pdf), which we discussed in lecture 5 (\"Observational studies\"), in order to draw conclusions from an observational study.\n",
    "\n",
    "We will work with a by-now classic dataset from Robert LaLonde's study \"[Evaluating the Econometric Evaluations of Training Programs](http://people.hbs.edu/nashraf/LaLonde_1986.pdf)\" (1986).\n",
    "The study investigated the effect of a job training program (\"National Supported Work Demonstration\") on the real earnings of an individual, a couple of years after completion of the program.\n",
    "Your task is to determine the effectiveness of the \"treatment\" represented by the job training program.\n",
    "\n",
    "\n",
    "#### Dataset description\n",
    "\n",
    "- `treat`: 1 if the subject participated in the job training program, 0 otherwise\n",
    "- `age`: the subject's age\n",
    "- `educ`: years of education\n",
    "- `race`: categorical variable with three possible values: Black, Hispanic, or White\n",
    "- `married`: 1 if the subject was married at the time of the training program, 0 otherwise\n",
    "- `nodegree`: 1 if the subject has earned no school degree, 0 otherwise\n",
    "- `re74`: real earnings in 1974 (pre-treatment)\n",
    "- `re75`: real earnings in 1975 (pre-treatment)\n",
    "- `re78`: real earnings in 1978 (outcome)\n",
    "\n",
    "If you want to brush up your knowledge on propensity scores and observational studies, we highly recommend Rosenbaum's excellent book on the [\"Design of Observational Studies\"](http://www.stewartschultz.com/statistics/books/Design%20of%20observational%20studies.pdf). Even just reading the first chapter (18 pages) will help you a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "import networkx as nx\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('lalonde.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. A naive analysis\n",
    "\n",
    "Compare the distribution of the outcome variable (`re78`) between the two groups, using plots and numbers.\n",
    "To summarize and compare the distributions, you may use the techniques we discussed in lectures 4 (\"Read the stats carefully\") and 6 (\"Data visualization\").\n",
    "\n",
    "What might a naive \"researcher\" conclude from this superficial analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "We divide the dataset into two groups: treated and non-treated (according to the indicator variable *treat*), and we try to evaluate whether there are differences in earning on the outcome variable (*re78*).\n",
    "\n",
    "In this step:\n",
    "- We compare the two groups and features associated with them (both with graphs and aggregate statistics).\n",
    "- We analyse the raw data by observing the mean and the median of the earnings, as well as the size of the dataset.\n",
    "- We try to gain more insight into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display non-zero count for each earning feature\n",
    "print('Amount of 0-salaries')\n",
    "(dataset[['re74', 're75', 're78']] == 0).describe().loc[['count', 'freq']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the table above, it appears that many earnings are 0. The value 0 means that the person was unemployed in that year. For instance, in 1978 (`re78`) only 471 out of 614 people were employed.\n",
    "\n",
    "For this reason, in our following analysis, we decided to split these features into two parts, and analyze them separately:\n",
    "- Employment flag: **true** if the person was employed in the year in question ('74, '75 or '78).\n",
    "- Actual earning: we plot the earning histogram/distribution only for non-zero values (that is, for people that were employed in the year in question).\n",
    "\n",
    "This allows us to interpret the data more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['employed74'] = dataset['re74'] > 0\n",
    "dataset['employed75'] = dataset['re75'] > 0\n",
    "dataset['employed78'] = dataset['re78'] > 0\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed with the actual analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define this helper function for analyzing the outcome variable. It will come handy throughout the rest of the homework.\n",
    "def analyse_earnings_distribution(dataset):\n",
    "    \"\"\" Split the data and analyse the earnings:\n",
    "    -Compute and plot the distributions.\n",
    "    -Compute the descriptive statistics to summarize the earnings.\n",
    "    \"\"\"\n",
    "    outcome_treated = dataset[dataset['treat'] == 1]['re78']\n",
    "    outcome_nontreated = dataset[dataset['treat'] == 0]['re78']\n",
    "    \n",
    "    outcome_treated_employed = outcome_treated[outcome_treated > 0]\n",
    "    outcome_nontreated_employed = outcome_nontreated[outcome_nontreated > 0]\n",
    "    \n",
    "    # Display counts (for the entire population)\n",
    "    description = pd.DataFrame()\n",
    "    description['Treated'] = outcome_treated.describe().loc[['count']].astype(int)\n",
    "    description['Non-treated'] = outcome_nontreated.describe().loc[['count']].astype(int)\n",
    "    display(description)\n",
    "    \n",
    "    # Plot normalized histogram (i.e. density) along with fitting line\n",
    "    bins = np.linspace(dataset['re78'].min(), dataset['re78'].max(), 30)\n",
    "    colors = sns.color_palette()\n",
    "    sns.distplot(outcome_treated_employed, kde=False, fit=stats.expon, bins=bins, color=colors[0], fit_kws={'color': colors[0]})\n",
    "    ax = sns.distplot(outcome_nontreated_employed, kde=False, fit=stats.expon, bins=bins, color=colors[1], fit_kws={'color': colors[1]})\n",
    "    ax.set_xlim(0,)\n",
    "    plt.grid()\n",
    "    plt.legend(['Treated', 'Non-treated'])\n",
    "    plt.title('Distribution of earnings (employed people)')\n",
    "    plt.xlabel('Earnings ($)')\n",
    "    plt.ylabel('Density')\n",
    "    plt.show()\n",
    "\n",
    "    # Display aggregate statistics for employed people\n",
    "    description = pd.DataFrame()\n",
    "    description['Earnings (treated)'] = outcome_treated_employed.describe()\n",
    "    description['Earnings (non-treated)'] = outcome_nontreated_employed.describe()\n",
    "    \n",
    "    description['Earnings (treated)'] = description['Earnings (treated)'].astype(\"int\")\n",
    "    description['Earnings (non-treated)'] = description['Earnings (non-treated)'].astype(\"int\")\n",
    "    display(HTML('<h3>Statistics of employed people</h3>'))\n",
    "    display(description)\n",
    "    \n",
    "    # Display aggregate statistics for unemployed people\n",
    "    display(HTML('<h3>Employment rates:</h3>'))\n",
    "    count_all = dataset.groupby(['treat'])['employed78'].count()\n",
    "    fractions = dataset.groupby(['treat'])['employed78'].sum()/count_all\n",
    "    display(HTML('<b>Treated group:</b> ' + '{0:.2f}'.format(fractions[0]) + '%'))\n",
    "    display(HTML('<b>Non-treated group:</b> ' + '{0:.2f}'.format(fractions[1]) + '%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_earnings_distribution(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- It appears that the two distributions are heavy-tailed (i.e they resemble a power law), and they do not seem to differ significantly. Since we are dealing with a heavy-tailed distribution, computing the mean does not make much sense since it is heavily affected by the few high values. Therefore, it is better to consider the more robust median to compare the earnings.\n",
    "- By inspecting the median, it seems that the non-treated group has slightly higher earnings than the treated group. However, we have to argue whether this difference is significant. In addition, we can see that the population sizes are unbalanced: we have more individuals in the non-treatment group (429) than the treatment group (185).\n",
    "\n",
    "**Conclusion:**\n",
    "Our naive analysis would lead us to the conclusion that there is not sufficient evidence for the treatment being effective in improving the income. However, as such, our argument is still weak. In the next part of the homework we will try to gain more evidence by doing a more in-depth analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. A closer look at the data\n",
    "\n",
    "You're not naive, of course (and even if you are, you've learned certain things in ADA), so you aren't content with a superficial analysis such as the above.\n",
    "You're aware of the dangers of observational studies, so you take a closer look at the data before jumping to conclusions.\n",
    "\n",
    "For each feature in the dataset, compare its distribution in the treated group with its distribution in the control group, using plots and numbers.\n",
    "As above, you may use the techniques we discussed in class for summarizing and comparing the distributions.\n",
    "\n",
    "What do you observe?\n",
    "Describe what your observations mean for the conclusions drawn by the naive \"researcher\" from his superficial analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "Before drawing any conclusion we have to assess whether the assignment of participants to the treatment is random, i.e. ideally it should not be possible to determine if an individual has been assigned to the treatment by just looking at their features.\n",
    "\n",
    "For this purpose we will first plot the distribution of all the feature of the individuals after dividing them into treated and non-treated groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features(dataset):\n",
    "    n_cols, n_rows = 2, 5\n",
    "    width, heigth = n_cols*5, n_rows*5\n",
    "    \n",
    "    fig, axs = plt.subplots(n_rows, n_cols)\n",
    "    fig.set_size_inches(width, heigth)\n",
    "    \n",
    "    treated = dataset[dataset['treat'] == 1]\n",
    "    nontreated = dataset[dataset['treat'] == 0]\n",
    "    \n",
    "    # Plot continuous features (along with their fitted density curve, using Kernel Density Estimation)\n",
    "    continuous = ['age', 'educ', 're74', 're75']\n",
    "    for i, feature in enumerate(continuous):\n",
    "        r, c = int(i/2), i%2\n",
    "        bins = np.linspace(dataset[feature].min(), dataset[feature].max(), 20)\n",
    "        sns.distplot(treated[feature][treated[feature] > 0], bins=bins, ax=axs[r][c])\n",
    "        sns.distplot(nontreated[feature][nontreated[feature] > 0], bins=bins, ax=axs[r][c])\n",
    "        axs[r][c].legend(['Treated', 'Non-treated'])\n",
    "        axs[r][c].set_ylabel('Density')\n",
    "        axs[r][c].set_xlim(0,)\n",
    "\n",
    "    # Plot categorical features\n",
    "    discretes = ['black', 'hispan', 'married', 'nodegree', 'employed74', 'employed75']\n",
    "    contrary = {'black': 'not black', 'hispan': 'not hispan', 'married': 'unmarried', 'nodegree': 'with degree',\n",
    "               'employed74': 'unemployed74', 'employed75': 'unemployed75'}\n",
    "    for i, feature in enumerate(discretes):\n",
    "        r, c = int(i/2)+2, i%2\n",
    "        count_all = dataset.groupby(['treat'])[feature].count()\n",
    "        (dataset.groupby(['treat', feature])[feature].count()/count_all).unstack()\\\n",
    "            .plot.bar(stacked=True, ax=axs[r][c])\n",
    "        \n",
    "\n",
    "        axs[r][c].legend([contrary[feature], feature], loc = 'lower center')\n",
    "        axs[r][c].set_xticklabels(['Non-treated', 'Treated'])\n",
    "        axs[r][c].set_ylabel('Proportion within group')\n",
    "        axs[r][c].set_xlabel('')\n",
    "        \n",
    "        for tick in axs[r][c].get_xticklabels():\n",
    "            tick.set_rotation(45)\n",
    "        \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_features(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations.** The most noticeable differences are in the following features:\n",
    "- **employed74, employed75**: the treated set had a higher portion of unemployed individuals during 1974 and 1975.\n",
    "- **black**: ~80% of the individuals in the treated set are black, as opposed to ~20% in the non-treated set.\n",
    "- **married**: ~20% of the individuals in the treated set are married, as opposed to ~50% in the non-treated set.\n",
    "\n",
    "**Conclusion.**\n",
    "\n",
    "Just by looking at the distributions of the features we notice great differences, which indicate that the assignment of the individuals to the treatment most likely was not random. The next task will help us quantify to what extent it is possible to predict whether an individual was been assigned to the treatment or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. A propensity score model\n",
    "\n",
    "Use logistic regression to estimate propensity scores for all points in the dataset.\n",
    "You may use `sklearn` to fit the logistic regression model and apply it to each data point to obtain propensity scores:\n",
    "\n",
    "```python\n",
    "from sklearn import linear_model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "```\n",
    "\n",
    "Recall that the propensity score of a data point represents its probability of receiving the treatment, based on its pre-treatment features (in this case, age, education, pre-treatment income, etc.).\n",
    "To brush up on propensity scores, you may read chapter 3.3 of the above-cited book by Rosenbaum or [this article](https://drive.google.com/file/d/0B4jctQY-uqhzTlpBaTBJRTJFVFE/view).\n",
    "\n",
    "Note: you do not need a train/test split here. Train and apply the model on the entire dataset. If you're wondering why this is the right thing to do in this situation, recall that the propensity score model is not used in order to make predictions about unseen data. Its sole purpose is to balance the dataset across treatment groups.\n",
    "(See p. 74 of Rosenbaum's book for an explanation why slight overfitting is even good for propensity scores.\n",
    "If you want even more information, read [this article](https://drive.google.com/file/d/0B4jctQY-uqhzTlpBaTBJRTJFVFE/view).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "We use *logistic regression* so as to assign to each individual a propensity score, which indicates the probability of being assigned to the treatment (that is, a value between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By specifying C=10^9, we are basically disabling L2 regularization (which cannot be turned off directly in sklearn)\n",
    "# We are not building a prediction model, so we do not care whether the model overfits\n",
    "logistic = linear_model.LogisticRegression(C=1e9)\n",
    "X = dataset[['age', 'educ', 'black', 'hispan', 'married', 'nodegree', 're74', 're75']]\n",
    "y = dataset['treat']\n",
    "logistic.fit(X, y)\n",
    "dataset['propensity'] = logistic.predict_proba(X)[:, 1]\n",
    "\n",
    "print(\"Accuracy of predictions:\", logistic.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This random subset shows that we could correctly predict whether these 5 individuals were assigned to the \\ntreatment.\")\n",
    "sample = dataset.sample(n=5, random_state=5)[[\"treat\", \"propensity\"]]\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "As expected, this step shows that with a simple linear model we could predict with 81% accuracy whether an individual was assigned to the treatment. Therefore, we have to balance the two sets before being able to draw credible conclusions.\n",
    "\n",
    "For the sake of completeness, we now show what would have happened if the subjects were assigned to random groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1000)\n",
    "y_random = np.random.randint(2, size=len(dataset)) # Assign random vector\n",
    "logistic.fit(X, y_random)\n",
    "print(\"Accuracy of predictions:\", logistic.score(X, y_random))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the accuracy is close to 50%, i.e. random. In reality, the classifier shows an accuracy slightly higher than 50% because it tends to overfit, but that is expected and does not represent a problem in our scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Balancing the dataset via matching\n",
    "\n",
    "Use the propensity scores to match each data point from the treated group with exactly one data point from the control group, while ensuring that each data point from the control group is matched with at most one data point from the treated group.\n",
    "(Hint: you may explore the `networkx` package in Python for predefined matching functions.)\n",
    "\n",
    "Your matching should maximize the similarity between matched subjects, as captured by their propensity scores.\n",
    "In other words, the sum (over all matched pairs) of absolute propensity-score differences between the two matched subjects should be minimized.\n",
    "\n",
    "After matching, you have as many treated as you have control subjects.\n",
    "Compare the outcomes (`re78`) between the two groups (treated and control).\n",
    "\n",
    "Also, compare again the feature-value distributions between the two groups, as you've done in part 2 above, but now only for the matched subjects.\n",
    "What do you observe?\n",
    "Are you closer to being able to draw valid conclusions now than you were before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "We use the *networkx* library to match the subject in the two groups. The procedure is as follows:\n",
    "- We build a bipartite graph, where the first set corresponds to the treated group, and the second set corresponds to the non-treated group.\n",
    "- We connect the nodes of the treated set to the nodes of the non-treated set. The edge weight corresponds to the absolute difference of their propensity scores.\n",
    "- We find the maximum-cardinality bipartite matching that also has the lowest total weight.\n",
    "- Finally, we keep only the matched subjects.\n",
    "\n",
    "**Important implementation detail:** networkx supports only max-weight matching, but we need min-weight matching. Fortunately, this optimization problem can be easily transformed by changing the sign of edge weights, because $\\min(f(x)) = -\\max(-f(x))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_weight_matching(dataset, max_weight=1):\n",
    "    \"\"\" Build a bipartite graph in which the nodes are the individuals. Individuals belonging two different sets\n",
    "    (treated and non-treated set) are connected by weighted edges. The weight is the absolute value of the \n",
    "    difference of the propensity scores of the two individuals. Only the edges whose weight \n",
    "    is under max_weight are kept. \n",
    "    Returns the datatset after dropping all the non-matched entries.\n",
    "    \"\"\"\n",
    "    \n",
    "    treated = dataset[dataset['treat'] == 1].index\n",
    "    nontreated = dataset[dataset['treat'] == 0].index\n",
    "\n",
    "    # Build bipartite graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for i in dataset.index:\n",
    "        G.add_node(i)\n",
    "\n",
    "    for i in treated:\n",
    "        for j in nontreated:\n",
    "            weight = abs(dataset['propensity'].loc[i] - dataset['propensity'].loc[j])\n",
    "            if weight < max_weight:\n",
    "                G.add_edge(i, j, weight=-weight) # Note the negative weight\n",
    "\n",
    "    # Find the optimal matching that also has maximum cardinality (i.e. number of matched nodes)\n",
    "    matching = nx.algorithms.max_weight_matching(G, maxcardinality=True)\n",
    "    \n",
    "    # Build the dataframe to show the distances of the matched points\n",
    "    matches = pd.DataFrame()\n",
    "    matches[\"id_1\"] = matching.keys()\n",
    "    matches[\"id_2\"] = matching.values()\n",
    "    prop1 = pd.Series(dataset.loc[matches[\"id_1\"]].propensity.tolist())\n",
    "    prop2 = pd.Series(dataset.loc[matches[\"id_2\"]].propensity.tolist())\n",
    "    matches[\"edge weight\"] = (prop1 - prop2).abs().round(decimals=4)\n",
    "    matches = matches.sort_values(by=\"edge weight\")\n",
    "    matches = matches[matches['id_1'] > matches['id_2']]\n",
    "\n",
    "    print('Found optimal matching with total weight', matches[\"edge weight\"].sum())\n",
    "    print('Number of matches:', matches.shape[0])\n",
    "    \n",
    "    print(\"These are the 5 best matches:\")\n",
    "    display(matches.head())\n",
    "    print(\"These are the 5 worse matches:\")\n",
    "    display(matches.tail())\n",
    "    \n",
    "    matches[\"edge weight\"].reset_index()[\"edge weight\"].hist(bins=50)\n",
    "    plt.title(\"Distribution of the matches\")\n",
    "    plt.ylabel(\"Number of matches\")\n",
    "    plt.xlabel(\"Weight of the edge\")\n",
    "\n",
    "    dataset_balanced = dataset.loc[matching.keys()]\n",
    "    return dataset_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_balanced = min_weight_matching(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, each subject of of the smallest group (treated) has been matched to exactly one subject of the non-treated group, for a total of 185+185 = 370 subjects.\n",
    "\n",
    "We now repeated the analysis of the outcome and of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_earnings_distribution(dataset_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the outcome distribution, our conclusion remains the same. There are no noticeable differences between the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_features(dataset_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "We can notice that the features are more equally distributed among the two sets, in particular **employed74** and **employed75** show less discrepancy in the number of individuals with 0 earnings than before. However, we are still not satisfied enough, since there are still datapoints which have been matched but do not have a very similar propensity score. This is due to the fact that the algorithm matches all the 185 entries in the treated set with 185 entries in in the other set, even if they have not a similar score. For example, we can notice that the distributions of the **black people** (and, to a smaller extent, age) are still unbalanced. We take care of this problem in the next task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Balancing the groups further\n",
    "\n",
    "Based on your comparison of feature-value distributions from part 4, are you fully satisfied with your matching?\n",
    "Would you say your dataset is sufficiently balanced?\n",
    "If not, in what ways could the \"balanced\" dataset you have obtained still not allow you to draw valid conclusions?\n",
    "\n",
    "Improve your matching by explicitly making sure that you match only subjects that have the same value for the problematic feature.\n",
    "Argue with numbers and plots that the two groups (treated and control) are now better balanced than after part 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "Since the **'black'** feature is still not balanced among the two sets, we decided to repeat the matching algorithm in a more constrained way. This time, we create an edge between two individuals only if their `black` feature is exactly the same (that is, if they are both black or both non-black)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated = dataset[dataset['treat'] == 1].index\n",
    "nontreated = dataset[dataset['treat'] == 0].index\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for i in dataset.index:\n",
    "    G.add_node(i)\n",
    "\n",
    "for i in treated:\n",
    "    for j in nontreated:\n",
    "        if dataset['black'].loc[i] == dataset['black'].loc[j]:\n",
    "            G.add_edge(i, j, weight=-abs(dataset['propensity'].loc[i] - dataset['propensity'].loc[j]))\n",
    "\n",
    "matching = nx.algorithms.max_weight_matching(G, maxcardinality=True)\n",
    "\n",
    "weight = 0\n",
    "for a, b in matching.items():\n",
    "    weight += G[a][b]['weight']\n",
    "print('Found optimal matching with total difference', -weight/2)\n",
    "dataset_balanced = dataset.loc[matching.keys()]\n",
    "print('Number of matches:', len(dataset_balanced)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, this means that we get a slightly lower population size (116+116 = 232 individuals), since we are allowing only certain matches.\n",
    "\n",
    "Again, we repeat the feature analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_features(dataset_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the two groups are now matched almost perfectly. To reinforce our argument, we train again our logistic regression model and we evaluate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = linear_model.LogisticRegression(C=1e9)\n",
    "X = dataset_balanced[['age', 'educ', 'black', 'hispan', 'married', 'nodegree', 're74', 're75']]\n",
    "y = dataset_balanced['treat']\n",
    "logistic.fit(X, y)\n",
    "\n",
    "print(\"Accuracy of predictions:\", logistic.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better than the previous result (81%). In fact, it is close to random (50%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. A less naive analysis\n",
    "\n",
    "Compare the outcomes (`re78`) between treated and control subjects, as you've done in part 1, but now only for the matched dataset you've obtained from part 5.\n",
    "What do you conclude about the effectiveness of the job training program?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_earnings_distribution(dataset_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Our last analysis reveals a mean increase in earnings of about 1900 dollars (median 1385), which contradicts the previous results. This seems in line with Dehejia & Wahba's research \"[Causal Effects in Nonexperimental Studies: Reevaluating the Evaluation of Training Programs](http://users.nber.org/~rdehejia/papers/dehejia_wahba_jasa.pdf)\", which reports a figure of 1700$.\n",
    "\n",
    "However, whether the job training program was effective is still a matter of debate. The employment rates do not seem to differ, and it should be assessed whether the difference in salary is statistically significant, given that we have a small sample. Our personal research on the topic presented us with contrasting opinions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 2: keep only the edges whose weight is under a threshold**\n",
    "For curiosity, we want to see what happens if we match only subjects that have a similar propensity score (by applying a limit), without manually matching individual features. We set the maximum difference to 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_balanced = min_weight_matching(dataset, max_weight=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a lower total weight, meaning that the matched datapoints are closer than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_features(dataset_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matching is similar to the one produced earlier. The features are well-balanced, except (maybe) for the unemployed indicator in 1974."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_earnings_distribution(dataset_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our conclusions are the same as before. The result seems identical to the last step of the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Applied ML\n",
    "\n",
    "We are going to build a classifier of news to directly assign them to 20 news categories. Note that the pipeline that you will build in this exercise could be of great help during your project if you plan to work with text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the 20newsgroup dataset. It is, again, a classic dataset that can directly be loaded using sklearn ([link](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)).  \n",
    "[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), short for term frequencyâ€“inverse document frequency, is of great help when if comes to compute textual features. Indeed, it gives more importance to terms that are more specific to the considered articles (TF) but reduces the importance of terms that are very frequent in the entire corpus (IDF). Compute TF-IDF features for every article using [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Then, split your dataset into a training, a testing and a validation set (10% for validation and 10% for testing). Each observation should be paired with its corresponding label (the article category).\n",
    "\n",
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the *20newsgroup* dataset using the built-in function by sklearn. Since we will split the dataset manually, we download everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_loaded = fetch_20newsgroups(subset='all')\n",
    "newsgroups_loaded['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of newsgroup posts divided by categories. In this dataset, 20 categories (shown above) are available. For each post in ```newsgroups['data'][i]```, we have the corresponding ground truth in ```newsgroups['target'][i]```, represented as an index between 0 and 19. The full name of the category can be retrieved simply by looking at ```newsgroups['target_name'][i]```.\n",
    "\n",
    "We now split the dataset into three groups:\n",
    "- **Training set (80%):** it will be used for training the model.\n",
    "- **Validation set (10%):** it will be used for experimenting with the model and tuning its hyperparameters, so as to obtain the best accuracy. The model will never be trained using this set.\n",
    "- **Test set (10%):** it will be evaluated only once, when the model is ready. This will determine the final accuracy of our model.\n",
    "\n",
    "This scheme allows us to evaluate whether the model generalizes or overfits the dataset. Generally speaking, we want our model to generalize well (that is, be able to classify unseen samples correctly) instead of just memorizing the input. A better approach would be to use *k-fold cross-validation*, i.e. split the dataset into $k$ parts, train it $k$ times on $k - 1$ parts, and validate it on the remaining parts. This would yield a more accurate estimate of the prediction error. However, we employed a static validation set since it is explicitly asked to do so (additionally, k-fold is more computationally expensive).\n",
    "\n",
    "Prior to splitting the dataset, we randomly shuffle the records. Throughout the rest of the homework, we will use a fixed seed (```random_state=0```) for all random operations, so as to ensure reproducibility of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = pd.DataFrame()\n",
    "newsgroups['data'] = newsgroups_loaded['data']\n",
    "newsgroups['target'] = newsgroups_loaded['target']\n",
    "shuffled = newsgroups.sample(frac=1, random_state=0)\n",
    "shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = shuffled.shape[0]\n",
    "newsgroups_train = shuffled[:int(n*0.8)]\n",
    "newsgroups_validation = shuffled[int(n*0.8):int(n*0.9)]\n",
    "newsgroups_test = shuffled[int(n*0.9):]\n",
    "print('# samples in the training set:', newsgroups_train.shape[0])\n",
    "print('# samples in the validation set:', newsgroups_validation.shape[0])\n",
    "print('# samples in the test set:', newsgroups_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed with converting each document to a feature vector. Machine learning algorithms require vector representations for their inputs, that is, each sample must be a point in $\\mathbb{R}^d$ space. Text, on the other hand, is not well suited to be represented as a vector, and therefore some form of pre-processing is required. The idea is that documents that are similar in content/semantics are also close among each other in their vector space.\n",
    "\n",
    "We perform the feature extraction using ```TfidfVectorizer```, which extracts *tf-idf* features. In a way that resembles a *bag-of-words* approach, words are assigned a weight that is proportional to the number of times they appear inside a document. Additionally, words that appear in many documents (e.g. \"this\", \"it\", \"the\", \"is\") are penalized.\n",
    "\n",
    "The algorithm works by first tokenizing each document (using ```CountVectorizer```), that is, extracting the words and getting the count for each word. Afterwards, features are transformed with ```TfidfTransformer```, which produces the final *tf-idf* features. The result is a sparse matrix that can be fed to a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_tr = vectorizer.fit_transform(newsgroups_train['data'])\n",
    "X_va = vectorizer.transform(newsgroups_validation['data'])\n",
    "X_te = vectorizer.transform(newsgroups_test['data'])\n",
    "\n",
    "y_tr = newsgroups_train['target'].values\n",
    "y_va = newsgroups_validation['target'].values\n",
    "y_te = newsgroups_test['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we fitted the vectorizer only on the training set, *after* splitting the dataset. The validation and test sets are transformed using the same model, as if they were unseen samples. This is the correct approach, since the full pipeline (vectorizer + classifier) must be trained using the training set alone. In a realistic scenario, the validation/test sets should only be used for evaluating the model. Fitting the vectorizer on the latter would bias the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that our dataset consists of 155842 features (a very sparse feature vector!)\n",
    "Each feature corresponds to a word, meaning that our dictionary contains 155842 words.\n",
    "Normally, we would limit the feature vector size to a small number (e.g. 100 to 1000 features) by selecting the words with the highest frequency. However, since we plan to use random forests, they are already able to do the splitting efficiently and do not require additional pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Train a random forest on your training set. Try to fine-tune the parameters of your predictor on your validation set using a simple grid search on the number of estimator \"n_estimators\" and the max depth of the trees \"max_depth\". Then, display a confusion matrix of your classification pipeline. Lastly, once you assessed your model, inspect the `feature_importances_` attribute of your random forest and discuss the obtained results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "We decided to keep the number of estimators constant, and run the grid search only on the maximum depth. As a rule of thumb, the number of estimators in a random forest should be set to a high value (e.g. 100 to 1000) and be no longer touched, since it can only improve the result. As described by the original authors of the algorithm, [\"you can run as many trees as you want\"](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#remarks). Many researchers claim that 100 to 300 estimators (i.e. trees) are more than enough for almost all cases. As a result, the choice of this hyperparameter is just a trade-off between training time and accuracy, given that after a certain threshold the accuracy no longer improves.\n",
    "\n",
    "Another advantage of having a large number of trees is that the result is more stable. Random forest use *bagging*, which means that the final result is the average of the outcomes of many simple models. If a low number of trees is used, the result will tend to fluctuate due to the high variance (and this could even compromise the validation result), whereas with many trees the result will be more consistent. This can be observed by running a k-fold cross validation (which is not done here): the higher the number of trees is, the lower the standard deviation of the error/accuracy will be.\n",
    "\n",
    "We set the number of estimators to 300, which requires a lot of training time, but produces a very stable estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_estimators = 300\n",
    "num_threads = multiprocessing.cpu_count()\n",
    "print('Random forests will be trained using', num_threads, 'CPU cores and', num_estimators, 'estimators')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal tree depth will be searched between 5 and 100 (in steps of 5). For each step, we train the model on the training set and evaluate the accuracy on the validation set. The aim, of course, is to maximize the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = np.arange(5, 105, 5)\n",
    "tr_accuracies = []\n",
    "va_accuracies = []\n",
    "for depth in depths:\n",
    "    clf = RandomForestClassifier(max_depth=depth, random_state=0, n_estimators=num_estimators, n_jobs=num_threads)\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    tr_accuracies.append(clf.score(X_tr, y_tr))\n",
    "    va_accuracies.append(clf.score(X_va, y_va))\n",
    "    print('Depth:', depth, tr_accuracies[-1], va_accuracies[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also plot the comparison between training error and validation error, according to the tree depth. While the validation accuracy is always lower than the training accuracy (which is normal), the trend reveals that a higher tree depth tends to improve the result (at least, in our scenario)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(depths[:len(tr_accuracies)], tr_accuracies)\n",
    "plt.plot(depths[:len(va_accuracies)], va_accuracies)\n",
    "plt.legend(['Training set', 'Validation set'])\n",
    "plt.ylabel('Accuracy')\n",
    "_ = plt.xlabel('Maximum tree depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the final model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_depth = depths[np.argmax(va_accuracies)]\n",
    "print('The best tree depth is', best_depth)\n",
    "clf = RandomForestClassifier(max_depth=best_depth, random_state=0, n_estimators=num_estimators, n_jobs=num_threads)\n",
    "clf.fit(X_tr, y_tr)\n",
    "print('Test accuracy:', clf.score(X_te, y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that the training error is higher than the validation/test error. This is normal, as the model tends to slightly overfit the training set. The important outcome is that the random forest generalizes well to unseen data, which seems the case.\n",
    "\n",
    "We now plot the so-called *confusion matrix*, which allows us to analyze the behaviour of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, cmap=plt.cm.Blues):\n",
    "    \n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title('Normalized confusion matrix', fontsize=20)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.tick_params(labelsize=15)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j],  '.2f'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=18)\n",
    "    plt.xlabel('Predicted label', fontsize=18)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_te, clf.predict(X_te))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(conf_matrix, classes=newsgroups_loaded['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the figure, the model seems to predict relatively well. Most errors are related to religion categories, which are closely related among each other.\n",
    "\n",
    "Another common practice in machine learning consists in interpreting the model, that is, visualizing the features that are learned, and how they affect each prediction label. In order to do this, we analyze the field `feature_importances_`, which, as the name suggests, provides us with the importance of each feature (a real value between 0 and 1 that represents the impact on the final prediction). In our case, each feature corresponds to a word, as returned by ```TfidfVectorizer```. Similarly, we can convert a feature back to a textual word by using the inverse map.\n",
    "We now show the 10 most important features of the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = pd.DataFrame(clf.feature_importances_).sort_values(by=0, ascending=False).head(10)\n",
    "\n",
    "mapping = vectorizer.get_feature_names()\n",
    "important_features.index = important_features.index.map(lambda x: mapping[x])\n",
    "important_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not very suggestive. We can see the words that have the highest impact on the model, but we cannot really interpret how they affect the result, as the model predicts 20 classes. We need to convert our interpretation problem to a binary classification problem. The solution is to build a [one-vs-rest](https://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest) model, that is, for each label $k$ we train a model where $y_i=1$ if $y_i==k$ and $0$ otherwise. This way, we can get useful feature importances for each category of posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "series = []\n",
    "for i, name in enumerate(newsgroups_loaded['target_names']):\n",
    "    clf = RandomForestClassifier(max_depth=best_depth, random_state=0, n_estimators=num_estimators, n_jobs=num_threads)\n",
    "    clf.fit(X_tr, y_tr == i)\n",
    "    important_features = pd.DataFrame(clf.feature_importances_).sort_values(by=0, ascending=False).head(10)\n",
    "    important_features.index = important_features.index.map(lambda x: mapping[x])\n",
    "    important_features.index.name = name\n",
    "    series.append(important_features.reset_index()[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now show the top 10 words for each category. We choose not to display the raw value of the importance, as they are not really easy to interpet. In simple words, if these words appear in a newsgroup post, they will tend to shift the attention of the classifier towards the respective class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(series, axis=1)\n",
    "result.index = range(1, 11)\n",
    "result.index.name = 'rank'\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The features certainly make sense. For instance, we can observe that categories related to religion contain words commonly associated with religion. Likewise, categories related to hardware/software contain domain-specific words."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
